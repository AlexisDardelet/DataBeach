{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40f97cd",
   "metadata": {},
   "source": [
    "Split des vidéo en set train, test et val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5095876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 vidéos points détectées.\n",
      "46 vidéos non-points détectées.\n",
      "Extraction frames pour TRAIN…\n",
      "Extraction frames pour VAL…\n",
      "Extraction frames pour TEST…\n",
      "CSV générés : train.csv, val.csv, test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "############################################\n",
    "# PARAMÈTRES\n",
    "############################################\n",
    "\n",
    "input_points = \"segments_points\"\n",
    "input_nonpoints = \"segments_temps_hors_jeu\"\n",
    "\n",
    "output_points = \"frames_points\"\n",
    "output_nonpoints = \"frames_nonpoints\"\n",
    "\n",
    "# ratios de split\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "############################################\n",
    "# FONCTION D'EXTRACTION DE FRAMES\n",
    "############################################\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    \"\"\"\n",
    "    Extrait les frames d'une vidéo dans un sous-dossier dédié.\n",
    "    Les images seront nommées frame_00001.jpg, frame_00002.jpg…\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", video_path,\n",
    "        \"-qscale:v\", \"2\",\n",
    "        os.path.join(output_folder, \"frame_%05d.jpg\"),\n",
    "        \"-y\"\n",
    "    ]\n",
    "\n",
    "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "\n",
    "############################################\n",
    "# RÉCUPÉRATION DES VIDÉOS\n",
    "############################################\n",
    "\n",
    "def list_videos(folder):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.mp4\")))\n",
    "\n",
    "videos_points = list_videos(input_points)\n",
    "videos_nonpoints = list_videos(input_nonpoints)\n",
    "\n",
    "print(f\"{len(videos_points)} vidéos points détectées.\")\n",
    "print(f\"{len(videos_nonpoints)} vidéos non-points détectées.\")\n",
    "\n",
    "\n",
    "############################################\n",
    "# SHUFFLE ET SPLIT\n",
    "############################################\n",
    "\n",
    "def split_list(lst, train_ratio, val_ratio):\n",
    "    random.shuffle(lst)\n",
    "    n = len(lst)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    train = lst[:n_train]\n",
    "    val = lst[n_train:n_train+n_val]\n",
    "    test = lst[n_train+n_val:]\n",
    "    return train, val, test\n",
    "\n",
    "points_train, points_val, points_test = split_list(videos_points, train_ratio, val_ratio)\n",
    "nonpoints_train, nonpoints_val, nonpoints_test = split_list(videos_nonpoints, train_ratio, val_ratio)\n",
    "\n",
    "############################################\n",
    "# EXTRACTION DES FRAMES\n",
    "############################################\n",
    "\n",
    "def process_split(videos, output_root, class_label):\n",
    "    csv_entries = []\n",
    "\n",
    "    for video in videos:\n",
    "        name = Path(video).stem  # exemple point_001\n",
    "        output_folder = os.path.join(output_root, name)\n",
    "        extract_frames(video, output_folder)\n",
    "\n",
    "        csv_entries.append((output_folder, class_label))\n",
    "\n",
    "    return csv_entries\n",
    "\n",
    "print(\"Extraction frames pour TRAIN…\")\n",
    "train_entries = []\n",
    "train_entries += process_split(points_train, output_points, 1)\n",
    "train_entries += process_split(nonpoints_train, output_nonpoints, 0)\n",
    "\n",
    "print(\"Extraction frames pour VAL…\")\n",
    "val_entries = []\n",
    "val_entries += process_split(points_val, output_points, 1)\n",
    "val_entries += process_split(nonpoints_val, output_nonpoints, 0)\n",
    "\n",
    "print(\"Extraction frames pour TEST…\")\n",
    "test_entries = []\n",
    "test_entries += process_split(points_test, output_points, 1)\n",
    "test_entries += process_split(nonpoints_test, output_nonpoints, 0)\n",
    "\n",
    "\n",
    "############################################\n",
    "# GÉNÉRATION DES CSV\n",
    "############################################\n",
    "\n",
    "def write_csv(filename, entries):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for folder, label in entries:\n",
    "            f.write(f\"{folder},{label}\\n\")\n",
    "\n",
    "write_csv(\"train.csv\", train_entries)\n",
    "write_csv(\"val.csv\", val_entries)\n",
    "write_csv(\"test.csv\", test_entries)\n",
    "\n",
    "print(\"CSV générés : train.csv, val.csv, test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80906dc8",
   "metadata": {},
   "source": [
    "# Entrainement du modèle (CNN + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6ec4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5912e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 1) DATASET POUR LES SÉQUENCES DE FRAMES\n",
    "##########################################\n",
    "\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour charger les frames pré-extraites d'une vidéo.\n",
    "    Échantillonnage possible pour limiter le nombre de frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, transform=None, frame_skip=2):\n",
    "        self.items = []\n",
    "        self.transform = transform\n",
    "        self.frame_skip = frame_skip  # toutes les 'frame_skip' frames\n",
    "\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                folder, label = line.strip().split(\",\")\n",
    "                self.items.append((folder, int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder, label = self.items[idx]\n",
    "\n",
    "        frame_files = sorted([f for f in os.listdir(folder) if f.lower().endswith(\".jpg\")])\n",
    "        frame_files = frame_files[::self.frame_skip]  # échantillonnage\n",
    "\n",
    "        frames = []\n",
    "        for f in frame_files:\n",
    "            img = Image.open(os.path.join(folder, f)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        video_tensor = torch.stack(frames)  # (T, 3, H, W)\n",
    "        return video_tensor, label\n",
    "\n",
    "##########################################\n",
    "# 2) COLLATE FN POUR PAD LES SÉQUENCES\n",
    "##########################################\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = [item[0] for item in batch]\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])\n",
    "    padded = pad_sequence(sequences, batch_first=True)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "##########################################\n",
    "# 3) MODELE CNN + LSTM\n",
    "##########################################\n",
    "\n",
    "class PointDetectorCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, num_classes=2):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]  # remove last fc\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        self.feature_dim = 512\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        feats = self.cnn(x)                 # (B*T, 512, 1, 1)\n",
    "        feats = feats.view(B, T, self.feature_dim)\n",
    "\n",
    "        packed = pack_padded_sequence(feats, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # dernière frame utile pour chaque séquence\n",
    "        last_outputs = torch.stack([output[i, length-1, :] for i, length in enumerate(lengths)])\n",
    "        return self.fc(last_outputs)\n",
    "\n",
    "##########################################\n",
    "# 4) HYPERPARAMS + DATALOADERS\n",
    "##########################################\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),  # réduit la mémoire\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = FrameSequenceDataset(\"train.csv\", transform, frame_skip=2)\n",
    "test_dataset  = FrameSequenceDataset(\"test.csv\", transform, frame_skip=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb46621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0 - Train loss: 0.3302\n",
      "Test accuracy: 68.75%\n",
      "Epoch 1 - Train loss: 0.0245\n",
      "Test accuracy: 100.00%\n",
      "Epoch 2 - Train loss: 0.0688\n",
      "Test accuracy: 87.50%\n",
      "Epoch 3 - Train loss: 0.0786\n",
      "Test accuracy: 81.25%\n",
      "Epoch 4 - Train loss: 0.0049\n",
      "Test accuracy: 87.50%\n",
      "Epoch 5 - Train loss: 0.0019\n",
      "Test accuracy: 87.50%\n",
      "Epoch 6 - Train loss: 0.0014\n",
      "Test accuracy: 87.50%\n",
      "Epoch 7 - Train loss: 0.0010\n",
      "Test accuracy: 87.50%\n",
      "Epoch 8 - Train loss: 0.0008\n",
      "Test accuracy: 87.50%\n",
      "Epoch 9 - Train loss: 0.0007\n",
      "Test accuracy: 87.50%\n",
      "Modèle sauvegardé : point_detector_lstm_gpu_optimized.pt\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 5) GPU SETUP\n",
    "##########################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = PointDetectorCNNLSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "##########################################\n",
    "# 6) TRAIN + EVAL FUNCTIONS\n",
    "##########################################\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for videos, lengths, labels in train_loader:\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(videos, lengths)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # libération mémoire GPU\n",
    "        del videos, labels, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch} - Train loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, lengths, labels in test_loader:\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(videos, lengths)\n",
    "            predicted = preds.argmax(dim=1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            del videos, labels, preds\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Test accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "##########################################\n",
    "# 7) TRAINING LOOP\n",
    "##########################################\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate()\n",
    "\n",
    "torch.save(model.state_dict(), \"point_detector_lstm_gpu_optimized.pt\")\n",
    "print(\"Modèle sauvegardé : point_detector_lstm_gpu_optimized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ff3ee",
   "metadata": {},
   "source": [
    "# Application du modèle sur une nouvelle vidéo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6ee52",
   "metadata": {},
   "source": [
    "Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb442aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d7fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointDetectorResNetBiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes=2, hidden_size=256, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Charger ResNet18 ou ResNet34 (les deux sont compatibles avec les clés que tu montres)\n",
    "        resnet = models.resnet18(weights=None)\n",
    "\n",
    "        # 2) On enlève la dernière FC, on garde tout le CNN\n",
    "        self.cnn = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,   # cnn.4.x dans tes clés\n",
    "            resnet.layer2,   # cnn.5.x\n",
    "            resnet.layer3,   # cnn.6.x\n",
    "            resnet.layer4    # cnn.7.x\n",
    "        )\n",
    "\n",
    "        # 3) Adaptive pooling → vecteur 512\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # 4) BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,    # ← obligatoire (clé reverse détectée)\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 5) Fully Connected  \n",
    "        # bidirectional => 2 * hidden_size\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "\n",
    "        # CNN\n",
    "        f = self.cnn(x)\n",
    "        f = self.pool(f)\n",
    "        f = f.view(B, T, -1)   # → (B,T,512)\n",
    "\n",
    "        # LSTM\n",
    "        out, _ = self.lstm(f)\n",
    "        out = out[:, -1]       # dernière frame\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c70e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK : modèle chargé correctement\n"
     ]
    }
   ],
   "source": [
    "model = PointDetectorResNetBiLSTM()\n",
    "state = torch.load(\"point_detector_lstm_gpu_optimized.pt\", map_location=\"cuda\",weights_only=True)\n",
    "model.load_state_dict(state)\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "print(\"OK : modèle chargé correctement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930603d",
   "metadata": {},
   "source": [
    "Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458d536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7783f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle CNN + LSTM pour la détection de points\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        # CNN = ResNet18\n",
    "        base = models.resnet18(weights=None)\n",
    "        self.cnn = nn.Sequential(*list(base.children())[:-1])\n",
    "        cnn_feature_dim = 512\n",
    "\n",
    "        # LSTM bidirectionnel, conforme au checkpoint\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_feature_dim,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)  # 256*2 pour bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        features = self.cnn(x)\n",
    "        features = features.view(B, T, -1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3362bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONCTION D'EXTRACTION DE FRAMES AVEC TRANSFERT GPU\n",
    "def extract_frames_gpu(video_path, device=\"cuda\", target_size=(224,224), max_frames=None):\n",
    "    \"\"\"\n",
    "    Extrait toutes les frames et les transfère sur le GPU.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame_tensor = transform(frame)\n",
    "        frames.append(frame_tensor)\n",
    "        count += 1\n",
    "        if max_frames and count >= max_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    frames_gpu = torch.stack(frames).to(device)\n",
    "    return frames_gpu, len(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420e64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des segments dans les prédictions\n",
    "def detect_segments(predictions):\n",
    "    \"\"\"\n",
    "    Détecte les segments consécutifs où prediction=1.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    in_segment = False\n",
    "    start = None\n",
    "\n",
    "    for i, p in enumerate(predictions):\n",
    "        if p == 1 and not in_segment:\n",
    "            in_segment = True\n",
    "            start = i\n",
    "        elif p == 0 and in_segment:\n",
    "            in_segment = False\n",
    "            end = i - 1\n",
    "            segments.append((start, end))\n",
    "\n",
    "    if in_segment:\n",
    "        segments.append((start, len(predictions)-1))\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e50462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habib\\AppData\\Local\\Temp\\ipykernel_17728\\530474211.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 93\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m detected_segments\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Exécution de l'inférence\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_streaming\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_video.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoint_detector_lstm_gpu_optimized.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_txt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_points_frames.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     99\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m segments\n",
      "Cell \u001b[1;32mIn[11], line 50\u001b[0m, in \u001b[0;36mrun_inference_streaming\u001b[1;34m(video_path, model_path, out_txt_path, threshold, chunk_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m         prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     48\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtotal_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     batch_frames \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Vide → libère RAM/Vram\u001b[39;00m\n\u001b[0;32m     53\u001b[0m frame_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def run_inference_streaming(\n",
    "    video_path,\n",
    "    model_path,\n",
    "    out_txt_path=\"new_points_frames.txt\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=200\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using:\", device)\n",
    "\n",
    "    # Charger modèle\n",
    "    model = CNN_LSTM().to(device)\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    # Préparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Impossible d’ouvrir la vidéo.\")\n",
    "\n",
    "    total_predictions = []  # toutes les classes frame par frame\n",
    "    frame_idx = 0\n",
    "    batch_frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Prépare la frame\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(frame)\n",
    "        tensor = transform(pil)\n",
    "        batch_frames.append(tensor)\n",
    "\n",
    "        # Quand chunk plein, on infère\n",
    "        if len(batch_frames) == chunk_size:\n",
    "            frames_gpu = torch.stack(batch_frames).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(frames_gpu)\n",
    "                prob = torch.softmax(logits, dim=1)[0, 1].cpu().numpy()\n",
    "            preds = (prob >= threshold).astype(int)\n",
    "\n",
    "            total_predictions.extend(preds.tolist())\n",
    "            batch_frames = []  # Vide → libère RAM/Vram\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    # Traiter les dernières frames restantes\n",
    "    if len(batch_frames) > 0:\n",
    "        frames_gpu = torch.stack(batch_frames).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(frames_gpu)\n",
    "            prob = torch.softmax(logits, dim=1)[0, 1].cpu().numpy()\n",
    "        preds = (prob >= threshold).astype(int)\n",
    "        total_predictions.extend(preds.tolist())\n",
    "\n",
    "    cap.release()\n",
    "    print(\"Total frames traitées :\", len(total_predictions))\n",
    "\n",
    "    # Détection segments\n",
    "    detected_segments = []\n",
    "    in_seg = False\n",
    "    start = None\n",
    "\n",
    "    for i, p in enumerate(total_predictions):\n",
    "        if p == 1 and not in_seg:\n",
    "            in_seg = True\n",
    "            start = i\n",
    "        elif p == 0 and in_seg:\n",
    "            in_seg = False\n",
    "            detected_segments.append((start, i - 1))\n",
    "\n",
    "    if in_seg:\n",
    "        detected_segments.append((start, len(total_predictions) - 1))\n",
    "\n",
    "    # Sauvegarde fichier txt\n",
    "    with open(out_txt_path, \"w\") as f:\n",
    "        for s, e in detected_segments:\n",
    "            f.write(f\"{s}-{e}\\n\")\n",
    "\n",
    "    print(f\"{len(detected_segments)} segments sauvegardés dans {out_txt_path}\")\n",
    "    return detected_segments\n",
    "\n",
    "\n",
    "# Exécution de l'inférence\n",
    "segments = run_inference_streaming(\n",
    "    video_path=\"new_video.mp4\",\n",
    "    model_path=\"point_detector_lstm_gpu_optimized.pt\",\n",
    "    out_txt_path=\"new_points_frames.txt\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=200\n",
    ")\n",
    "\n",
    "segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\habib\\AppData\\Local\\Temp\\ipykernel_17728\\636685660.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Exécution de l'inférence\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_streaming\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_video.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoint_detector_lstm_gpu_optimized.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_txt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_points_frames.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m segments\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mrun_inference_streaming\u001b[1;34m(video_path, model_path, out_txt_path, threshold, chunk_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m         prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     48\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtotal_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     batch_frames \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Vide → libère RAM/Vram\u001b[39;00m\n\u001b[0;32m     53\u001b[0m frame_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
